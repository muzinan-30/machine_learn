## 线性回归
### 单变量线性回归 
> linear-regression with one variable

x - f - y-hat

loss function cost function

均方差成本函数

梯度下降：Gradient descent algorithm
不要进行同步更新

```python
tmp_w = w - alpha * 代价函数对w的偏导数
tmp_b = b - alpha * 代价函数对b的偏导数
w = tmp_w
b = tmp_b
```
凸函数总是存在全局最小值，选取合适的学习率就能到达指定的地方
代价函数(平均误差成本函数)的值最小：使用梯度回归 or 正规方程
### 多变量线性回归
> linear-regression with multiple variable

数据的标准化：normalization
x = x-avg/std
学习率的选择：learning rate
- 如果学习率 α 过小，则达到收敛所需的迭代次数会非常高；
- 如果学习率 α 过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。

先观察数据，然后选择适合的模型，推解出向量

注意：对于不可逆的矩阵 X（通常因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用。

一对多：
一个变量为负变量，其他为正变量，进行拟合

代价函数：
为了防止过拟合，目的就是消除高次项的数据，减少系数的大小，并且在代价函数中有所体现，
通过这个代价函数选择出的 θ3 和 θ4 对预测结果的影响就比之前要小许多。假如有非常多的特征，并不知道哪些特征我们要惩罚，可以对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：

