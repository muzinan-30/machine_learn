## 逻辑回归
### Hypothesis 表示
> 逻辑回归的输出变量范围始终在0和1之间。 逻辑回归模型的假设是： hθ = g(θTX) ，
> 其中： X 代表特征向量 g 代表逻辑函数（Logistic function）。

```python
import numpy as np
def sigmod(z):
    return 1/(1+np.exp(-z))
```

当 hθ(x) >= 0.5 时，预测 y=1。

当 hθ(x) < 0.5 时，预测 y=0 。


对于线性回归模型，我们定义的代价函数是所有模型误差的平方和（ J(θ)=1/(2m) Σ (hθ(x(i))-y(i))2 ）。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将 hθ(x)=(1+e-θTx)-1 带入到这样定义了的代价函数中时，
我们得到的代价函数将是一个非凸函数（non-convex function）。
非凸函数有很多的局部最小值，梯度下降算法会受到影响。

```python
import numpy as np

def sigmoid(z):
  return 1/(1 + np.exp(-z))

def cost1(theta, X, y): # 这是第一种方法
  first = - y.T @ np.log(sigmoid(X @ theta))
  second = (1 - y.T) @ np.log(1 - sigmoid(X @ theta))
  return ((first - second) / (len(X))).item()

def cost2(theta, X, y): # 这是第二种方法
  first = np.multiply(-y, np.log(sigmoid(X @ theta)))
  second = np.multiply((1 - y), np.log(1 - sigmoid(X @ theta)))
  return np.sum(first - second) / (len(X))
```
梯度下降并不是唯一使用的算法，还有其他一些更高级、更复杂的算法。例如共轭梯度法（Conjugate gradient）、BFGS (变尺度法) 和L-BFGS (限制变尺度法) 就是其中更高级的优化算法，它们需要你计算代价函数 J(θ) 和导数项，然后会它们帮你最小化代价函数


这Conjugate Gradient、BFGS、L-BFGS等算法有许多优点：

1. 不需要手动选择学习率 α。只用给出计算导数项和代价函数的方法，因为算法内有一个智能的内部循环，称为线性搜索(line search)算法，它可以自动尝试不同的学习速率 ，并自动选择一个好的学习速率 ，它甚至可以为每次迭代选择不同的学习速率。
2. 这些算法实际上在做更复杂的事情，不仅仅是选择一个好的学习速率，所以它们往往最终比梯度下降收敛得快多了，不过关于它们到底做什么的详细讨论，已经超过了这里讨论的范围。

